{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AK_Mammogram_Analysis_Challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Brogr4mmer/Week1_public/blob/master/AK_Mammogram_Analysis_Challenge.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "w9iIELyqJo0T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Mammogram Analysis Challenge\n",
        "\n",
        "A mammogram is an X-ray of the breast used primarily by radiologists to detect signs of breast cancer like abnormal masses and calcifications (signs of a possible tumor). Sometimes these abnormalities are obvious, sometimes they are not. In younger women especially, masses and calcifications can be difficult for radiologists to detect in a mammogram due to dense surrounding tissue. This makes mammograms a great candidate for machine learning analysis. A program that can detect abnormalities with high accuracy could be used to catch radiologists' mistakes and potentially save lives. \n",
        "\n",
        "For this challenge you will classify mammogram segments into two classes (positive for an abnormality and negative). You may also attempt to use the \"complex labels\" to classify the mammograms into 4 classes (negative, positive benign calcification, positive malignant calcification, positive benign mass, and positive malignant mass). We recommend you begin with the simple labels, but try both if you have time!\n",
        "\n",
        "Below we provide you with code to visualize the dataset as well as a simple but functional model that does a relatively poor job at this classification problem. It's up to you to build a classifier that does better! Your models will be evaluated based on confusion matrix , Aread under the ROC Curve, and Creativity.\n",
        "\n",
        "Before you run any code make sure you turn GPU support on! (Edit > Notebook Settings > Hardware Accelerator)."
      ]
    },
    {
      "metadata": {
        "id": "eNtIxtyy5qS-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run this cell to import the packages you will need to unpack the dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import scipy\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import random\n",
        "import pickle\n",
        "import os\n",
        "import zipfile\n",
        "import scipy.ndimage\n",
        "from google.colab import files\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MWwqOpa9Kroz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "This dataset is from the publicly available Digital Database for Screening Mammography (DDSM) and the Curated Breast Imaging Subset of DDSM (CBIS-DDSM).\n",
        "\n",
        "The simple labels are 0 (negative for an abnormality) and 1 (positive for an abnormality). The complex labels are:\n",
        "* 0 = Negative\n",
        "* 1 = Benign Calcification\n",
        "* 2 = Benign Mass\n",
        "* 3 = Malignant Calcification\n",
        "* 4 = Malignant Mass\n",
        "\n",
        "\n",
        "\n",
        "Run the cell below to get all the data for this challenge. After running the cell, the following data will be loaded into the following variables:\n",
        "* **train_simple_labels**: 2 column pandas df where each row contains a label (0 or 1) and the unique ID of the image the label corresponds to. 5500 samples/rows.\n",
        "* **train_complex_labels**: 2 column pandas df where the rows contains a label (0, 1, 2, 3, 4) and the unique ID of the image the label corresponds to. 5500 samples/rows.\n",
        "* **train_images_df**: 2 column pandas df where the rows contain a 299x299 numpy array of the (greyscale) pixel values in the image and the unique ID of the image. 5500 samples/rows. \n",
        "* **test_images_df**: 2 column pandas df where the rows contain a 299x299 numpy array of the (greyscale) pixel values in the image and the unique ID of the image. 500 samples/rows.\n",
        "\n",
        "DO NOT USE THE TEST IMAGES IN MODEL DEVELOPMENT AT ALL. These images will be used to evaluate your model and should only be run through the classifier ONCE at the time of submission. You will not have access to the test image labels.\n",
        "\n",
        "Explore the variables listed above! \n"
      ]
    },
    {
      "metadata": {
        "id": "hxDEIp63AZNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c309fb4-a3d7-4d04-8d84-ce5a2c5d93a6"
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/BeaverWorksMedlytics/Week3_public.git  # Don't rerun these two lines!\n",
        "os.chdir('Week3_public/Challenge/Data')                               # Don't rerun these two lines!"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Week3_public' already exists and is not an empty directory.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vvoo9zFr5rTW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "train_simple_labels = pd.read_csv('train_simple_labels.csv', header = None)\n",
        "train_complex_labels = pd.read_csv('train_complex_labels.csv', header=None)\n",
        "eval_simple_labels = pd.read_csv('eval_simple_labels.csv', header=None)\n",
        "eval_complex_labels = pd.read_csv('eval_complex_labels.csv', header=None)\n",
        "\n",
        "train_simple_labels = pd.concat([train_simple_labels, eval_simple_labels], axis=0, ignore_index = True)\n",
        "train_complex_labels = pd.concat([train_complex_labels, eval_complex_labels], axis=0, ignore_index = True)\n",
        "Labels = pd.DataFrame(list(range(5500)))\n",
        "train_simple_labels = pd.concat([train_simple_labels, Labels], axis=1)\n",
        "train_simple_labels.columns = ['Label', 'Unique_Index']\n",
        "train_complex_labels = pd.concat([train_complex_labels, Labels], axis=1)\n",
        "train_complex_labels.columns = ['Label', 'Unique_Index']\n",
        "\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile('Mamm_Images_Train.zip.zip', 'r')\n",
        "zip_ref.extractall('Mamm_Images_Train')\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile('Mamm_Images_Eval_zip.zip', 'r')\n",
        "zip_ref.extractall('Mamm_Images_Eval')\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile('Mamm_Images_Test.zip.zip', 'r')\n",
        "zip_ref.extractall('Mamm_Images_Test')\n",
        "zip_ref.close()  \n",
        "\n",
        "\n",
        "train_images = []\n",
        "for i in range(5000):\n",
        "  im = scipy.ndimage.imread('Mamm_Images_Train/Mamm_Images_Train/image' + str(i) + '.jpg')\n",
        "  train_images.append(im)\n",
        "  \n",
        "for h in range(500):\n",
        "  im = scipy.ndimage.imread('Mamm_Images_Eval/Mamm_Images_Eval/image' +str(h) + '.jpg')\n",
        "  train_images.append(im)\n",
        "  \n",
        "train_images_df = pd.DataFrame([train_images])\n",
        "train_images_df = train_images_df.transpose()\n",
        "Labels = pd.DataFrame(list(range(5500)))\n",
        "train_images_df = pd.concat([train_images_df, Labels], axis = 1)\n",
        "train_images_df.columns = ['Images', 'Unique Index']\n",
        "\n",
        "\n",
        "test_images = []\n",
        "for k in range(1500):\n",
        "  im = scipy.ndimage.imread('Mamm_Images_Test/Mamm_Images_Test/image' + str(k) + '.jpg')\n",
        "  test_images.append(im)\n",
        "  \n",
        "  \n",
        "test_images_df = pd.DataFrame([test_images])\n",
        "test_images_df = test_images_df.transpose()\n",
        "Labels = pd.DataFrame(list(range(5500, 7000)))\n",
        "test_images_df = pd.concat([test_images_df, Labels], axis = 1)\n",
        "test_images_df.columns = ['Images', 'Unique Index']\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bkcCHU6dOqtw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Visualization\n",
        "\n",
        "Run the cell below this one to see what the images you are working with look like. They are 299x299 pixel greyscale images of sections of mammograms. Generally, masses and calcifications show up as blobs of various size, shape, and brightness. As mentioned already, dense tissue shows up brighter on mammograms and it can be dificult to spot an abnormality in mammograms with denser tissue. Each image is labeled with its simple and complex labels. Notice that some images are not tissue at all but instead are segments of the mammogram labels (image 10 for example). Further data preprocessing or a robust classifier should successfully deal with these images. Change the numbers in the range function to look at other images in the training dataset.\n"
      ]
    },
    {
      "metadata": {
        "id": "exvqx8rZhFuG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    \n",
        "    coord = tf.train.Coordinator()\n",
        "    threads = tf.train.start_queue_runners(coord=coord)\n",
        "        \n",
        "    for i in range(0, 10): # change these numbers to explore the training dataset further\n",
        "      plt.imshow(255 - train_images[i])\n",
        "      plt.title(\"label_simple: \" + str(train_simple_labels.iloc[i,0]) + '  label_complex: ' + str(train_complex_labels.iloc[i,0]))\n",
        "      plt.grid(b=None)\n",
        "      plt.show()\n",
        "            \n",
        "    coord.request_stop()\n",
        "    \n",
        "    coord.join(threads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t5AU6_9kyuWg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(np.array(train_images), train_simple_labels['Label'].values, test_size = 0.2)\n",
        "\n",
        "#reshape X_train and X_val to 2D\n",
        "num_images, nrows, ncols = X_train.shape\n",
        "X_train = X_train.reshape(num_images,ncols*nrows)\n",
        "\n",
        "num_images, nrows, ncols = X_val.shape\n",
        "X_val = X_val.reshape(num_images,ncols*nrows)\n",
        "\n",
        "\n",
        "\n",
        "#normalize the data\n",
        "X_train = X_train.astype('float')/255\n",
        "X_val = X_val.astype('float')/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UoWrYB9QAmNu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### formatting the test data\n",
        "\n",
        "X_test = np.array(test_images)\n",
        "\n",
        "### normalize the data\n",
        "X_test = X_test.astype('float')/255\n",
        "\n",
        "###reshape the data\n",
        "num_images, nrows, ncols = X_test.shape\n",
        "X_test = X_test.reshape(num_images,ncols*nrows)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KRrX9BgOzXRx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8ce6d301-c7a7-49c1-8b4c-9e5cdc982834"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth = 5, criterion='gini')\n",
        "clf = clf.fit(X_train, y_train)\n",
        "t = clf.predict_proba(X_val)\n",
        "t\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35978992, 0.64021008],\n",
              "       [0.85325973, 0.14674027],\n",
              "       [0.46449185, 0.53550815],\n",
              "       ...,\n",
              "       [0.73698726, 0.26301274],\n",
              "       [0.08273054, 0.91726946],\n",
              "       [0.20140993, 0.79859007]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "FHWQbJd98Oqc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2e96242-236e-4cd9-d7bd-af433f06ad61"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# This is exctly the first metric you'll be evaluated on!\n",
        "# Note: this will only work on the binary case -- let us know if you get to the multi-class case\n",
        "\n",
        "def cm_metric(y_true,y_prob):\n",
        "    \n",
        "    # predict the class with the greatest probability\n",
        "    y_val_predict = [np.argmax(y) for y in y_prob]\n",
        "\n",
        "    # calculate the confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_val_predict)\n",
        "\n",
        "    #cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    #print(cm_norm)\n",
        "    return sum(sum(np.multiply(cm,np.array([[2, -3], [-6, 2]])))), cm\n",
        "\n",
        "metric, y = cm_metric(y_val,t)\n",
        "print('Confusion Matrix Metric: ', metric)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix Metric:  863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T68RDJSw7PRU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "667fa30e-955f-4b32-e1ec-7d2bf3864936"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "fpr, tpr, threshold = roc_curve(y_val, t[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('AUC: ',roc_auc)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC:  0.8917190082644627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fu9645i-75Ww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "d9af2d14-2129-47f6-ac27-b0a179b9e541"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAFnCAYAAABU0WtaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEMKKSGMXQgghrIg0diGEEMKK/A8nUaMM74k2vAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fa77080fda0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "J0XbZkruvgDk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "ca95024c-285e-4b17-d394-fb2cd1a9c33f"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.imshow(y, cmap='hot', interpolation='nearest')\n",
        "plt.grid(False)\n",
        "plt.colorbar()\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAF3CAYAAADdKPCAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns5QERUTEZykJioiIz/r/AVYEjwKbsTsIAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fa76d286d30>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "8mYW1M6qAUGH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a0cb039b-f945-44f3-c911-5f054f188230"
      },
      "cell_type": "code",
      "source": [
        "q = clf.predict_proba(X_test)\n",
        "q[3:5]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.46323648, 0.53676352],\n",
              "       [0.6143384 , 0.3856616 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "y94Kd-yp3Ii3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        },
        "outputId": "bab4b8e6-2934-4989-bcd4-d04285560876"
      },
      "cell_type": "code",
      "source": [
        "q = clf.predict_proba(X_test)\n",
        "testing_output = pd.DataFrame(q, columns = ['zero', 'one'])\n",
        "Labels = pd.DataFrame(list(range(5500, 7000)))\n",
        "testing_output['Unique indices'] = Labels\n",
        "\n",
        "hector = []\n",
        "for i in range(q.shape[0]):\n",
        "#   if q[i][0] > q[i][1]:\n",
        "#     hector.append(0)\n",
        "#   else:\n",
        "#     hector.append(1)\n",
        "  hector.append(np.argmax([testing_output['zero'][i], testing_output['one'][i]], axis = 0))\n",
        "testing_output['guesses'] = hector\n",
        "# testing_output"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>zero</th>\n",
              "      <th>one</th>\n",
              "      <th>Unique indices</th>\n",
              "      <th>guesses</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.807042</td>\n",
              "      <td>0.192958</td>\n",
              "      <td>5500</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.946183</td>\n",
              "      <td>0.053817</td>\n",
              "      <td>5501</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.938662</td>\n",
              "      <td>0.061338</td>\n",
              "      <td>5502</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.463236</td>\n",
              "      <td>0.536764</td>\n",
              "      <td>5503</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.614338</td>\n",
              "      <td>0.385662</td>\n",
              "      <td>5504</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.697681</td>\n",
              "      <td>0.302319</td>\n",
              "      <td>5505</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.933260</td>\n",
              "      <td>0.066740</td>\n",
              "      <td>5506</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.944939</td>\n",
              "      <td>0.055061</td>\n",
              "      <td>5507</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.492867</td>\n",
              "      <td>0.507133</td>\n",
              "      <td>5508</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.537677</td>\n",
              "      <td>0.462323</td>\n",
              "      <td>5509</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.660978</td>\n",
              "      <td>0.339022</td>\n",
              "      <td>5510</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.916589</td>\n",
              "      <td>0.083411</td>\n",
              "      <td>5511</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.948411</td>\n",
              "      <td>0.051589</td>\n",
              "      <td>5512</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.724710</td>\n",
              "      <td>0.275290</td>\n",
              "      <td>5513</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.948411</td>\n",
              "      <td>0.051589</td>\n",
              "      <td>5514</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.943871</td>\n",
              "      <td>0.056129</td>\n",
              "      <td>5515</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.948411</td>\n",
              "      <td>0.051589</td>\n",
              "      <td>5516</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.946183</td>\n",
              "      <td>0.053817</td>\n",
              "      <td>5517</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.383432</td>\n",
              "      <td>0.616568</td>\n",
              "      <td>5518</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.495918</td>\n",
              "      <td>0.504082</td>\n",
              "      <td>5519</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.948411</td>\n",
              "      <td>0.051589</td>\n",
              "      <td>5520</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.298532</td>\n",
              "      <td>0.701468</td>\n",
              "      <td>5521</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.948411</td>\n",
              "      <td>0.051589</td>\n",
              "      <td>5522</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.943109</td>\n",
              "      <td>0.056891</td>\n",
              "      <td>5523</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.919722</td>\n",
              "      <td>0.080278</td>\n",
              "      <td>5524</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.513623</td>\n",
              "      <td>0.486377</td>\n",
              "      <td>5525</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.948411</td>\n",
              "      <td>0.051589</td>\n",
              "      <td>5526</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.126795</td>\n",
              "      <td>0.873205</td>\n",
              "      <td>5527</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.890543</td>\n",
              "      <td>0.109457</td>\n",
              "      <td>5528</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.472667</td>\n",
              "      <td>0.527333</td>\n",
              "      <td>5529</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1470</th>\n",
              "      <td>0.062622</td>\n",
              "      <td>0.937378</td>\n",
              "      <td>6970</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1471</th>\n",
              "      <td>0.603141</td>\n",
              "      <td>0.396859</td>\n",
              "      <td>6971</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1472</th>\n",
              "      <td>0.104697</td>\n",
              "      <td>0.895303</td>\n",
              "      <td>6972</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1473</th>\n",
              "      <td>0.099812</td>\n",
              "      <td>0.900188</td>\n",
              "      <td>6973</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1474</th>\n",
              "      <td>0.286799</td>\n",
              "      <td>0.713201</td>\n",
              "      <td>6974</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1475</th>\n",
              "      <td>0.416863</td>\n",
              "      <td>0.583137</td>\n",
              "      <td>6975</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1476</th>\n",
              "      <td>0.089876</td>\n",
              "      <td>0.910124</td>\n",
              "      <td>6976</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1477</th>\n",
              "      <td>0.302200</td>\n",
              "      <td>0.697800</td>\n",
              "      <td>6977</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1478</th>\n",
              "      <td>0.785426</td>\n",
              "      <td>0.214574</td>\n",
              "      <td>6978</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1479</th>\n",
              "      <td>0.126288</td>\n",
              "      <td>0.873712</td>\n",
              "      <td>6979</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1480</th>\n",
              "      <td>0.635619</td>\n",
              "      <td>0.364381</td>\n",
              "      <td>6980</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1481</th>\n",
              "      <td>0.064147</td>\n",
              "      <td>0.935853</td>\n",
              "      <td>6981</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1482</th>\n",
              "      <td>0.137746</td>\n",
              "      <td>0.862254</td>\n",
              "      <td>6982</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1483</th>\n",
              "      <td>0.106479</td>\n",
              "      <td>0.893521</td>\n",
              "      <td>6983</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1484</th>\n",
              "      <td>0.300743</td>\n",
              "      <td>0.699257</td>\n",
              "      <td>6984</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1485</th>\n",
              "      <td>0.065718</td>\n",
              "      <td>0.934282</td>\n",
              "      <td>6985</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1486</th>\n",
              "      <td>0.049616</td>\n",
              "      <td>0.950384</td>\n",
              "      <td>6986</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1487</th>\n",
              "      <td>0.105564</td>\n",
              "      <td>0.894436</td>\n",
              "      <td>6987</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1488</th>\n",
              "      <td>0.514291</td>\n",
              "      <td>0.485709</td>\n",
              "      <td>6988</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1489</th>\n",
              "      <td>0.095913</td>\n",
              "      <td>0.904087</td>\n",
              "      <td>6989</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1490</th>\n",
              "      <td>0.699152</td>\n",
              "      <td>0.300848</td>\n",
              "      <td>6990</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1491</th>\n",
              "      <td>0.442573</td>\n",
              "      <td>0.557427</td>\n",
              "      <td>6991</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1492</th>\n",
              "      <td>0.129715</td>\n",
              "      <td>0.870285</td>\n",
              "      <td>6992</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1493</th>\n",
              "      <td>0.248097</td>\n",
              "      <td>0.751903</td>\n",
              "      <td>6993</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1494</th>\n",
              "      <td>0.108929</td>\n",
              "      <td>0.891071</td>\n",
              "      <td>6994</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>0.065198</td>\n",
              "      <td>0.934802</td>\n",
              "      <td>6995</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>0.930085</td>\n",
              "      <td>0.069915</td>\n",
              "      <td>6996</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>0.118442</td>\n",
              "      <td>0.881558</td>\n",
              "      <td>6997</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>0.059067</td>\n",
              "      <td>0.940933</td>\n",
              "      <td>6998</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>0.590294</td>\n",
              "      <td>0.409706</td>\n",
              "      <td>6999</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1500 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          zero       one  Unique indices  guesses\n",
              "0     0.807042  0.192958            5500        0\n",
              "1     0.946183  0.053817            5501        0\n",
              "2     0.938662  0.061338            5502        0\n",
              "3     0.463236  0.536764            5503        1\n",
              "4     0.614338  0.385662            5504        0\n",
              "5     0.697681  0.302319            5505        0\n",
              "6     0.933260  0.066740            5506        0\n",
              "7     0.944939  0.055061            5507        0\n",
              "8     0.492867  0.507133            5508        1\n",
              "9     0.537677  0.462323            5509        0\n",
              "10    0.660978  0.339022            5510        0\n",
              "11    0.916589  0.083411            5511        0\n",
              "12    0.948411  0.051589            5512        0\n",
              "13    0.724710  0.275290            5513        0\n",
              "14    0.948411  0.051589            5514        0\n",
              "15    0.943871  0.056129            5515        0\n",
              "16    0.948411  0.051589            5516        0\n",
              "17    0.946183  0.053817            5517        0\n",
              "18    0.383432  0.616568            5518        1\n",
              "19    0.495918  0.504082            5519        1\n",
              "20    0.948411  0.051589            5520        0\n",
              "21    0.298532  0.701468            5521        1\n",
              "22    0.948411  0.051589            5522        0\n",
              "23    0.943109  0.056891            5523        0\n",
              "24    0.919722  0.080278            5524        0\n",
              "25    0.513623  0.486377            5525        0\n",
              "26    0.948411  0.051589            5526        0\n",
              "27    0.126795  0.873205            5527        1\n",
              "28    0.890543  0.109457            5528        0\n",
              "29    0.472667  0.527333            5529        1\n",
              "...        ...       ...             ...      ...\n",
              "1470  0.062622  0.937378            6970        1\n",
              "1471  0.603141  0.396859            6971        0\n",
              "1472  0.104697  0.895303            6972        1\n",
              "1473  0.099812  0.900188            6973        1\n",
              "1474  0.286799  0.713201            6974        1\n",
              "1475  0.416863  0.583137            6975        1\n",
              "1476  0.089876  0.910124            6976        1\n",
              "1477  0.302200  0.697800            6977        1\n",
              "1478  0.785426  0.214574            6978        0\n",
              "1479  0.126288  0.873712            6979        1\n",
              "1480  0.635619  0.364381            6980        0\n",
              "1481  0.064147  0.935853            6981        1\n",
              "1482  0.137746  0.862254            6982        1\n",
              "1483  0.106479  0.893521            6983        1\n",
              "1484  0.300743  0.699257            6984        1\n",
              "1485  0.065718  0.934282            6985        1\n",
              "1486  0.049616  0.950384            6986        1\n",
              "1487  0.105564  0.894436            6987        1\n",
              "1488  0.514291  0.485709            6988        0\n",
              "1489  0.095913  0.904087            6989        1\n",
              "1490  0.699152  0.300848            6990        0\n",
              "1491  0.442573  0.557427            6991        1\n",
              "1492  0.129715  0.870285            6992        1\n",
              "1493  0.248097  0.751903            6993        1\n",
              "1494  0.108929  0.891071            6994        1\n",
              "1495  0.065198  0.934802            6995        1\n",
              "1496  0.930085  0.069915            6996        0\n",
              "1497  0.118442  0.881558            6997        1\n",
              "1498  0.059067  0.940933            6998        1\n",
              "1499  0.590294  0.409706            6999        0\n",
              "\n",
              "[1500 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "j_-QKjxx-NqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pickle.dump(testing_output, open('Ai3_binary.p', 'wb')) # Creates pickle file in Google Colab but doesn't save to your machine\n",
        "files.download('Ai3_binary.p') # Saving to pickle file to local drive (should be in Downloads). Send pickle file to beaverworksmedlytics@gmail.com\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NLPmjL4x6a_8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def testing(test_images):   # Final function to output pandas df of ordered image indexes and their respective guesses\n",
        "  mostly_blank_feature_list = mostly_blank_feature(test_images)\n",
        "  true_rows_index, false_rows_index = partition_test(mostly_blank_feature_list, 1)\n",
        "  \n",
        "  true_rows_guess = np.array([0] * len(true_rows_index))\n",
        "  false_rows_guess = np.array([1] * len(false_rows_index))\n",
        "  \n",
        "  guesses = np.concatenate((true_rows_guess, false_rows_guess))\n",
        "  guesses_one_hot = np.eye(2)[guesses] # converting raw guesses (0 or one) to one hot vectors where 0 = [1, 0] and 1 = [0,1] \n",
        "  indexes = np.concatenate((np.array(true_rows_index), np.array(false_rows_index)))\n",
        "  \n",
        "  output = pd.DataFrame(data={'Guesses': guesses, 'Zero': guesses_one_hot[:,0], 'One': guesses_one_hot[:,1], 'Indexes': indexes}) # creating pandas dataframe of 3 comlumns \n",
        "                                                                                                                                  # (indices and probabilities for each class)\n",
        "  output = output.sort_values('Indexes') # sorting so that the column for image indexes are ordered from 0-499 and they stay with their respective Guesses\n",
        "  \n",
        "  output = output.reset_index(drop=True) # Reseting dataframe indices so that they are ordered 0-499\n",
        "  \n",
        "  output = output.drop('Indexes', axis = 1)  # Because df indices and images indices columns are now the same we can drop the indexes column\n",
        "  \n",
        "  cols = output.columns.tolist()   # Converting columns to lists so they can be reordered as \"Guest, Zero, One\"\n",
        "  \n",
        "  output = output[cols[:1] + cols[2:] + cols[1:2]]  # Reordering columns (you don't have to do this but I wanted to)\n",
        "  \n",
        "  output = pd.concat([output, pd.DataFrame(test_images_df.iloc[:, 1])], axis =1)\n",
        "  \n",
        "  return(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MHUUxXcCt8_i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Example Classifier\n",
        "\n",
        "Below is an example of a very poor classifier for this dataset. It is a decision tree with just one feature - whether or not the image is mostly blank space or not. It performs with about 72.4% accuracy on the eval data and 64.4% accuracy on the test data. Remember that accuracy is not always the best measure of a classifier!\n",
        "\n",
        "It's up to you to build a classifier (of any variety) that does a better job!"
      ]
    },
    {
      "metadata": {
        "id": "GJEnbuKNhHse",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#######       Example Classifier          ##########\n",
        "\n",
        "all_training = pd.concat([train_images_df, train_simple_labels], axis = 1)  ## combining images df and label dfs to make one datafram\n",
        "all_training =  all_training.reindex(np.random.permutation(all_training.index)).drop(['Unique Index'], axis = 1) ## shuffling/randomizing df (Note: I use 0:4000 for training and 4000:5500 for validation)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mostly_blank_feature(images):  # Feature Extraction\n",
        "  mbf = []\n",
        "  # Sample 2000 of 89401 pixels. If half of them are less than 100 (fairly dark) then mostly_blank_feature = 1, else it's 0\n",
        "  for h in range(len(images)):\n",
        "    if(sum(np.array(random.sample(list(images[h].reshape([299*299])), 2000)) < 100) > 1000): \n",
        "      mbf.append(1)\n",
        "    else:\n",
        "      mbf.append(0)\n",
        "  return(mbf)\n",
        "\n",
        "\n",
        "def gini(labels):  # Gini Impurity measures how often an element would be incorrectly labeled if it were labeled according to a particular partition\n",
        "  classes = 2      # (used to measure how good the partition is)\n",
        "  impurity = 1\n",
        "  \n",
        "  for label in range(classes):\n",
        "    prob_of_label = sum(labels)/float(len(labels))\n",
        "    impurity -= prob_of_label**2\n",
        "  return impurity\n",
        "\n",
        "def partition(feature_list, label_list, feature_value):  # Splits the images by according to some feature value\n",
        "  true_rows, false_rows = [], []\n",
        "  true_rows_index, false_rows_index = [], []\n",
        "  for d in range(len(feature_list)):\n",
        "    if(feature_list[d] == feature_value):\n",
        "      true_rows.append(label_list[d])\n",
        "      true_rows_index.append(d)\n",
        "    else:\n",
        "      false_rows.append(label_list[d])\n",
        "      false_rows_index.append(d)\n",
        "\n",
        "  return true_rows, false_rows, true_rows_index, false_rows_index\n",
        "\n",
        "def info_gain(trues, falses, current):  # Measures how much info is gained by partitioning the data in a particular way using Gini Impurity\n",
        "  p = float(len(trues)) / (len(trues) + len(falses))\n",
        "  return current - p * gini(trues) - (1 - p) * gini(falses)\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bKdqRpN-3FTk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b46da42-0fd3-422f-faa9-1859f95b829a"
      },
      "cell_type": "code",
      "source": [
        "d = {'train_simple_labels': all_training.iloc[0:4000,1].values.reshape(4000), 'mostly_blank_feature': np.array(mostly_blank_feature(all_training.iloc[0:4000,0].values))} \n",
        "df = pd.DataFrame(d)\n",
        "\n",
        "true_rows, false_rows, true_rows_index, false_rows_index = partition(list(df['mostly_blank_feature']), list(df['train_simple_labels']), 1)\n",
        "info_gain(false_rows, true_rows, gini(list(df['train_simple_labels'])))   # finding if the partition increases information. It (barely) does, \n",
        "                                                                          # so I will use this partition"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1090753572574053"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "metadata": {
        "id": "W376I5cCMDQ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tree(eval_images, eval_simple_labels):       # Making tree function that takes Validation data and outputs accuracy\n",
        "  \n",
        "  mostly_blank_feature_list = mostly_blank_feature(eval_images)\n",
        "  true_rows, false_rows, true_rows_index, false_rows_index = partition(mostly_blank_feature_list, eval_simple_labels, 1)\n",
        "  \n",
        "  true_rows_guess = np.array([0] * len(true_rows))\n",
        "  false_rows_guess = np.array([1] * len(false_rows))\n",
        "    \n",
        "  correct = 0  \n",
        "  for i in range(len(true_rows)):\n",
        "    if(true_rows_guess[i] == true_rows[i]):\n",
        "      correct += 1\n",
        "  for h in range(len(false_rows)):\n",
        "    if(false_rows_guess[h] == false_rows[h]):\n",
        "      correct += 1\n",
        "    \n",
        "  return('accuracy: ' + str(correct/len(eval_simple_labels)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QK4R7jAcXK_-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11c1f4c8-8255-41c7-908c-70c66b53287d"
      },
      "cell_type": "code",
      "source": [
        "tree(all_training.iloc[4000:5500, 0].values, all_training.iloc[4000:5500, 1].values) # Evaluating accuracy on validation data with known labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'accuracy: 0.724'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "__heFUJZrwiJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def partition_test(feature_list, feature_value): \n",
        "  \n",
        "  true_rows_index, false_rows_index = [], []      # New partition function without using labels (only using indexes to keep track of images because test labels are \n",
        "                                                  # not available)\n",
        "  for d in range(len(feature_list)):\n",
        "    if(feature_list[d] == feature_value):\n",
        "      true_rows_index.append(d)\n",
        "    else:\n",
        "      false_rows_index.append(d)\n",
        "\n",
        "  return true_rows_index, false_rows_index\n",
        "\n",
        "\n",
        "####### Look at the function below to see how I got the Pandas dataframe into the right format   #######\n",
        "\n",
        "def testing(test_images):   # Final function to output pandas df of ordered image indexes and their respective guesses\n",
        "  mostly_blank_feature_list = mostly_blank_feature(test_images)\n",
        "  true_rows_index, false_rows_index = partition_test(mostly_blank_feature_list, 1)\n",
        "  \n",
        "  true_rows_guess = np.array([0] * len(true_rows_index))\n",
        "  false_rows_guess = np.array([1] * len(false_rows_index))\n",
        "  \n",
        "  guesses = np.concatenate((true_rows_guess, false_rows_guess))\n",
        "  guesses_one_hot = np.eye(2)[guesses] # converting raw guesses (0 or one) to one hot vectors where 0 = [1, 0] and 1 = [0,1] \n",
        "  indexes = np.concatenate((np.array(true_rows_index), np.array(false_rows_index)))\n",
        "  \n",
        "  output = pd.DataFrame(data={'Guesses': guesses, 'Zero': guesses_one_hot[:,0], 'One': guesses_one_hot[:,1], 'Indexes': indexes}) # creating pandas dataframe of 3 comlumns \n",
        "                                                                                                                                  # (indices and probabilities for each class)\n",
        "  output = output.sort_values('Indexes') # sorting so that the column for image indexes are ordered from 0-499 and they stay with their respective Guesses\n",
        "  \n",
        "  output = output.reset_index(drop=True) # Reseting dataframe indices so that they are ordered 0-499\n",
        "  \n",
        "  output = output.drop('Indexes', axis = 1)  # Because df indices and images indices columns are now the same we can drop the indexes column\n",
        "  \n",
        "  cols = output.columns.tolist()   # Converting columns to lists so they can be reordered as \"Guest, Zero, One\"\n",
        "  \n",
        "  output = output[cols[:1] + cols[2:] + cols[1:2]]  # Reordering columns (you don't have to do this but I wanted to)\n",
        "  \n",
        "  output = pd.concat([output, pd.DataFrame(test_images_df.iloc[:, 1])], axis =1)\n",
        "  \n",
        "  return(output)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "epp8wL3Vu-vD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "d6e91150-2ea3-4405-a156-92ac32699420"
      },
      "cell_type": "code",
      "source": [
        "output = testing(test_images)\n",
        "pickle.dump(output, open('filename.p', 'wb')) # Creates pickle file in Google Colab but doesn't save to your machine\n",
        "files.download('filename.p') # Saving to pickle file to local drive (should be in Downloads). Send pickle file to beaverworksmedlytics@gmail.com\n",
        "output"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6f7b8185db31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filename.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Creates pickle file in Google Colab but doesn't save to your machine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filename.p'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Saving to pickle file to local drive (should be in Downloads). Send pickle file to beaverworksmedlytics@gmail.com\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'testing' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ultum1UWTdG7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Submitting Your Model\n",
        "\n",
        "Once you have finished, run your classifier on the test images and make a pickle file containing a pandas dataframe. Exporting a pickle file to your local drive is demonstrated in the example classifier. \n",
        "\n",
        "* First, create a pandas dataframe with 4 columns (or 7 columns for complex lables). One column should contain your best guess at the class of the image and one column should contain the unique index for that image. The other columns should contain the probability that the image is classified in each class. **Make sure the unique indices on your dataframe match the unique indices of the images being predicted. The order of rows and columns do not matter, as long as there are 1500 rows and either 4 or 7 columns (depending on which classifier you have made)**. Run through the example classifier provided to see what the output pandas df looks like if you are confused.\n",
        " * If your classifier does not compute the probabilities for each class and simply labels with a 0 or 1 (or 0, 1, 2, 3, 4 for complex labels), the probability for whichever class is predicted is 1.00 and the probability for the other class(es) is(are) 0.00.\n",
        "* Then download the pandas dataframe as a pickle file using pickle.dump() and files.download().\n",
        "* Lastly, email the pickle file to beaverworksmedlytics@gmail.com to submit it for evaluation.\n",
        " * You can submit up to 3 times before your final evaluation\n",
        "\n",
        "Your model will be evaluated on Confusion Matrix Score, Area Under the ROC curve (ROCAUC), and creativity. There will be a \"winning\" group for each of these categories.\n"
      ]
    },
    {
      "metadata": {
        "id": "3S-gd_slKJLc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# How Your Model Will Be Evaluated\n",
        "\n",
        " \n",
        "* **Confusion Matrix Score**: You will receive a certain number of points for each correct classification and a certain point penalty for each incorrect classification. The point reward scheme is shown below for both binary and multiclass classifiers. This reward scheme punishes less crucial misclassifications (misclassifying a Negative scan as a Benign Calcification) much less than it punishes costly misclassifications (misclassifying a Malgnant Calcification as a Negative). In this way, our evaluation metric reflects the practical needs of the classifier in its application to real-world problems. It is our hope that you keep these differential punishments in mind when creating your classifiers. \n",
        "\n",
        "\n",
        "Truth_Col | 0 (Neg) | 1 (BC) | 2 (BM) | 3 (MC) | 4 (MM) |\n",
        "--- | ---\n",
        "0 (Neg) | +2 | -1 | -1 | -3 | -3 |\n",
        "1 (BC) | -2 | +2 | -1 | -3 |-3 |\n",
        "2 (BM)| -2 | -1 | +2 | -3 | -3 |\n",
        "3 (MC) | -6 | -4 | -4 | +2 | -2 | \n",
        "4 (MM) | -6 | -4 | -4 | -2 | +2 | \n",
        "\n",
        "\n",
        "Truth_Col | 0 (Neg) | 1 (Pos)\n",
        "--- | ---\n",
        "0 (Neg) | +2 | -3\n",
        "1 (Pos) |  -6 | +2\n",
        "\n",
        "\n",
        "\n",
        "* **Area Under ROC Curve:** The receiver operating characteristic (ROC) curve plots the true positive rate (sensitivity/recall) against the false positive rate (fall-out) at many decision threshold settings. The area under the curve (AUC) measures discrimination, the classifier's ability to correctly identify samples from the \"positive\" and \"negative\" cases. Intuitively, AUC is the probability that a randomly chosen \"positive\" sample will be labeled as \"more positive\" than a randomly chosen \"negative\" sample. In the case of a multi-class ROC curve, each class is considered separately before taking the weighted average of all the class results. Simply put, the class under consideration is labeled as \"positive\" while all other classes are labeled as \"negative.\" The AUCROC score should be between 0.5 and 1, in which 0.5 is random classification and 1 is perfect classification.\n",
        " * ![alt text](http://fastml.com/images/auc/roc_curve.png)\n",
        " * In the image above, the dashed line represents a classifier that classifies based purely on chance. The perfect classifier will have a line that passes through the top left corner of the graph (true positive rate = 1, false positive rate = 0) and will have a ROCAUC (ROC Area Under Curve) of 1. \n",
        " \n",
        "\n",
        " \n",
        " Using the test data, the example classifier above performs thus on these metrics:\n",
        "  * Confusion Matrix Score: 706\n",
        "  * ROCAUC: 0.737\n",
        "  * Creativity: ZERO!\n",
        "  \n",
        "  ![alt text](https://github.com/BeaverWorksMedlytics/Week3_public/blob/master/download111.png?raw=true)\n",
        "  \n",
        "   ### Below there is some pseudocode showing how we will calculate these three metrics when evaluating your classifier."
      ]
    },
    {
      "metadata": {
        "id": "O3HyjrjKoYBp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Confusion Matrix Score for Binary Classifier\n",
        "\n",
        "binary_confmat_score = np.array([[2, -3],\n",
        "                                [-6, 2]])\n",
        "\n",
        "confmat = confusion_matrix(Ground_Truth_Labels, Predicted_Labels) # confusion_matrix is a function from sklearn.metrics and it automatically\n",
        "                                                                  # constructs a confusion matrix for any number of classes \n",
        "                                                                  # Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "  \n",
        "confmat_score = np.sum(confmat * binary_confmat_score)\n",
        "\n",
        "\n",
        "# Confusion Matrix Score for Multiclass Classifier\n",
        "\n",
        "multiclass_confmat_score = np.array([[2,-1,-1,-3,-3],\n",
        "                                    [-2,2,-1,-3,-3],\n",
        "                                    [-2,-1,2,-3,-3],\n",
        "                                    [-6,-4,-4,2,-2],\n",
        "                                    [-6,-4,-4,-2,2]])\n",
        "\n",
        "confmat = confusion_matrix(Ground_Truth_Labels, Predicted_Labels)\n",
        "  \n",
        "confmat_score = np.sum(confmat * binary_confmat_score)\n",
        "\n",
        "\n",
        "      \n",
        "# ROCAUC for Binary Classifier\n",
        "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(Ground_Truth_Labels, Predicted_Labels, pos_label=1)\n",
        "roc_auc = metrics.auc(false_positive_rate, true_positive_rate)    # roc_curve() and auc() are also functions from sklearn.metrics. roc_curve calculates true and flase positive\n",
        "                                                                  # rates over many theshold values. pos_label = 1 specifies that in the labels a 1\n",
        "                                                                  # denotes a positive example (and anything less denotes a negative example). auc() simply calculates the AUC.\n",
        "                                                                  # Documentation:\n",
        "                                                                  # roc_curve(): http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
        "                                                                  # auc(): http://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n",
        "\n",
        "# ROCAUC for Multiclass Classifier\n",
        "  # ROCAUC for multiclass classifiers are calculated by treating each class as a binary classification (this class or all other classes) and micro-averaging \n",
        "  # each class's ROCAUC to get a total ROCAUC\n",
        "false_positive_rat = dict()\n",
        "true_positive_rate = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(5):  # looping through each of 5 classes and getting false and true positive rates and calculating ROCAUC for each class\n",
        "  fales_positive_rate[i], true_positive_rate[i], _ = roc_curve(Ground_Truth_Labels_OneHot, Prediction_Probabilities) # Because labels are not binary, \n",
        "  roc_auc[i] = auc(false_positive_rate[i], true_positive_rate[i])                                                    # they need to be One Hot Vectors (ex. 2 = [0,0,1,0,0])\n",
        "                                                                                                                     # Prediction_Probabilities are the calculated probabilites\n",
        "                                                                                                                     # that the example belongs to each of the 5 classes\n",
        "false_positive_rate[\"micro\"], true_positive_rate[\"micro\"], _ = roc_curve(Ground_Truth_Labels_OneHot.ravel(), Prediction_Probabilities.ravel())\n",
        "roc_auc = auc(false_positive_rate[\"micro\"], true_positive_rate[\"micro\"]) # micro-averaging averages the ROCAUCs from each class weighted \n",
        "                                                                         # according to number of example in that class\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mkJcTaUvc2uG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Troubleshooting Colab\n",
        "\n",
        "- If you get an \"ResourcesExhausted\" error, this means you have used all the GPU resources dedicated to your session and you will need to restart your runtime (Runtime > Restart Runtime). After restarting your runtime your environment will be wiped so you will need to reload the data.\n",
        "\n",
        "- If you get a warning window that says you are close to reaching the session's memory limit, this means you are nearing a ResourcesExhausted error. You can still run code until the error occurs, but note that you will likely need to restart your runtime soon.\n",
        "\n",
        "- ResourcesExhausted errors will only occur if you're using GPU support.\n",
        "\n",
        "- Google Colab will end your runtime automatically after 24 hours\n",
        "\n",
        "- If things seem really slow, double check that you're using the GPU"
      ]
    }
  ]
}